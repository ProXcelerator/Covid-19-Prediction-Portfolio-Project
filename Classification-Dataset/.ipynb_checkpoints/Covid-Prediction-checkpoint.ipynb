{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lung Prediction Project\n",
    "\n",
    "### Project Scope:\n",
    "In this project we are looking at images of x-ray scans of 3 different lung types(Normal, Covid, and Pneunomia).   \n",
    "I will perform a CNN to see if I can make accurate predictions on the test set of images. \n",
    "\n",
    "### Project Steps:\n",
    "- Import librarys \n",
    "- Seperate the training data and test data\n",
    "- build the CNN model \n",
    "- Set up the metrics and optimizations \n",
    "- Train the model \n",
    "- Test the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Taylor\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 251 images belonging to 3 classes.\n",
      "Found 78 images belonging to 3 classes.\n",
      "(16, 256, 256, 1) (16, 3)\n"
     ]
    }
   ],
   "source": [
    "# create train image generator with a scale of values from 0-255\n",
    "generator = ImageDataGenerator(rescale=1./255,zoom_range=0.2,\n",
    "        rotation_range=15,\n",
    "        width_shift_range=0.05,\n",
    "        height_shift_range=0.05)\n",
    "\n",
    "batch_size= 16\n",
    "# get file from training directory\n",
    "train_iterator = generator.flow_from_directory('Covid19-dataset/train', class_mode = 'categorical', color_mode = 'grayscale', batch_size=batch_size, target_size=(256,256))\n",
    "# create test image generator with a scale of values from 0-255\n",
    "test_generator = generator = ImageDataGenerator(rescale=1./255)\n",
    "# get file from test directory\n",
    "test_iterator = test_generator.flow_from_directory('Covid19-dataset/test', class_mode = 'categorical', color_mode = 'grayscale', batch_size=batch_size, target_size=(256,256))\n",
    "# get the input and labels information\n",
    "sample_batch_input,sample_batch_labels  = train_iterator.next()\n",
    "# pring the shapes of the input and labels\n",
    "print(sample_batch_input.shape, sample_batch_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential model\n",
    "model = Sequential()\n",
    "# Add an input layer with a specified shape\n",
    "model.add(layers.Input(shape=(256, 256, 1)))\n",
    "# Add a convolutional layer with 2 filters, 5x5 kernel, ReLU activation, valid padding, and stride of 2\n",
    "model.add(layers.Conv2D(2, 5, activation='relu', padding='valid', strides=2))\n",
    "# Add a max pooling layer with a pool size of 5x5 and stride of 2 in both dimensions\n",
    "model.add(layers.MaxPooling2D(pool_size=(5, 5), strides=(2, 2)))\n",
    "# Add another convolutional layer with 4 filters, 3x3 kernel, ReLU activation, valid padding, and stride of 2\n",
    "model.add(layers.Conv2D(4, 3, activation='relu', padding='valid', strides=2))\n",
    "# Add another max pooling layer with a pool size of 2x2 and stride of 2 in both dimensions\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
    "# Flatten the output from the previous layer to prepare for fully connected layers\n",
    "model.add(layers.Flatten())\n",
    "# Add a dense layer with 200 units and ReLU activation function\n",
    "model.add(layers.Dense(200, activation='relu'))\n",
    "# Add the output layer with 3 units (for classification) and softmax activation\n",
    "model.add(layers.Dense(3, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimization and Model Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 126, 126, 2)       52        \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 61, 61, 2)         0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 30, 30, 4)         76        \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 15, 15, 4)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 900)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 200)               180200    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 603       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 180931 (706.76 KB)\n",
      "Trainable params: 180931 (706.76 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# set the optimizer of the model and the optimizers learning rate\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.005)\n",
    "\n",
    "# compile the model using categorical crossentropy because we have more than two output labels. \n",
    "model.compile(optimizer=opt , loss='categorical_crossentropy', metrics=[tf.keras.metrics.CategoricalAccuracy(), tf.keras.metrics.AUC()])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "16/15 [==============================] - ETA: 0s - loss: 0.1568 - categorical_accuracy: 0.9323 - auc: 0.9930INFO:tensorflow:Assets written to: Covid19-dataset/model\\checkpoint-best-accuracy\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Covid19-dataset/model\\checkpoint-best-accuracy\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 2s 149ms/step - loss: 0.1568 - categorical_accuracy: 0.9323 - auc: 0.9930 - val_loss: 0.1223 - val_categorical_accuracy: 0.9487 - val_auc: 0.9960\n",
      "Epoch 2/50\n",
      "16/15 [==============================] - ETA: 0s - loss: 0.1216 - categorical_accuracy: 0.9562 - auc: 0.9960INFO:tensorflow:Assets written to: Covid19-dataset/model\\checkpoint-best-accuracy\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: Covid19-dataset/model\\checkpoint-best-accuracy\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 2s 153ms/step - loss: 0.1216 - categorical_accuracy: 0.9562 - auc: 0.9960 - val_loss: 0.1217 - val_categorical_accuracy: 0.9487 - val_auc: 0.9956\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 2s 110ms/step - loss: 0.1412 - categorical_accuracy: 0.9402 - auc: 0.9942 - val_loss: 0.2200 - val_categorical_accuracy: 0.8846 - val_auc: 0.9864\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 2s 118ms/step - loss: 0.2771 - categorical_accuracy: 0.9044 - auc: 0.9803 - val_loss: 0.1364 - val_categorical_accuracy: 0.9615 - val_auc: 0.9940\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 2s 108ms/step - loss: 0.1831 - categorical_accuracy: 0.9323 - auc: 0.9907 - val_loss: 0.1247 - val_categorical_accuracy: 0.9487 - val_auc: 0.9965\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 2s 104ms/step - loss: 0.2068 - categorical_accuracy: 0.9203 - auc: 0.9881 - val_loss: 0.1660 - val_categorical_accuracy: 0.9231 - val_auc: 0.9924\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 2s 103ms/step - loss: 0.2120 - categorical_accuracy: 0.9203 - auc: 0.9872 - val_loss: 0.1790 - val_categorical_accuracy: 0.9103 - val_auc: 0.9917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1e81ccdc2e0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stop the model early if it gets a low loss value on the validation data\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# save the model at the lowest loss on the validation data\n",
    "checkpoint_path = 'Covid19-dataset/model/checkpoint-best-accuracy'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath = checkpoint_path,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    save_best_only=True,\n",
    ")\n",
    "\n",
    "# fit the model \n",
    "model.fit(train_iterator, \n",
    "        steps_per_epoch = train_iterator.samples/batch_size, \n",
    "        epochs = 50, batch_size=batch_size, \n",
    "        validation_data = test_iterator,\n",
    "        validation_steps = test_iterator.samples/batch_size,\n",
    "        callbacks = [early_stop, model_checkpoint_callback]\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 32ms/step - loss: 0.1582 - categorical_accuracy: 0.9359 - auc: 0.9933\n",
      "Test accuracy: 0.9358974099159241\n",
      "{'Covid': 0, 'Normal': 1, 'Pneumonia': 2}\n"
     ]
    }
   ],
   "source": [
    "# evaluate the loss, accuracy, AUC\n",
    "loss, acc, auc = model.evaluate(test_iterator)\n",
    "print('Test accuracy:', acc)\n",
    "# model.save('Covid19-dataset/model') # if you want to save the model uncomment and set your directory location\n",
    "\n",
    "# see how the model is labeling the classes so we can know how to look at the predictions\n",
    "classes = train_iterator.class_indices\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model and Test New Unseen Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5/5 [==============================] - 0s 36ms/step - loss: 0.0225 - categorical_accuracy: 1.0000 - auc_10: 1.0000\n",
      "Test accuracy: 1.0\n",
      "AUC Score:  1.0\n",
      "LOSS Score:  0.02254110760986805\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "Predicted class: Normal\n",
      "Raw predictions: [[1.1007418e-06 9.9971598e-01 2.8286045e-04]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "import numpy as np\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model('Covid19-dataset/model')\n",
    "\n",
    "# evaluate the loss, accuracy, AUC \n",
    "loss, acc, auc = model.evaluate(test_iterator)\n",
    "print('Test accuracy:', acc)\n",
    "print('Test AUC Score: ', auc)\n",
    "print('LOSS Score: ', loss)\n",
    "\n",
    "\n",
    "# Specify the path to the image file\n",
    "img_path = \"Covid19-dataset/test/Normal/1.png\"\n",
    "# Load the image, resizing it to the target size of (256, 256) and using grayscale color mode\n",
    "img = image.load_img(img_path, target_size=(256, 256), color_mode='grayscale')\n",
    "# Convert the image to a NumPy array\n",
    "img_array = image.img_to_array(img)\n",
    "# Add an extra dimension to the array to represent the batch size (1 in this case)\n",
    "img_array = np.expand_dims(img_array, axis=0)\n",
    "# Normalize pixel values to the range [0, 1]\n",
    "img_array = img_array / 255.0\n",
    "\n",
    "\n",
    "# predict what the image is \n",
    "predictions = model.predict(img_array)\n",
    "\n",
    "# label the classes so we know what we are predicting \n",
    "class_labels = ['Covid', 'Normal', 'Pneumonia'] \n",
    "# Find the index with the highest predicted probability in the array\n",
    "predicted_class_index = np.argmax(predictions)\n",
    "# Map the predicted index to the corresponding class label using class_labels\n",
    "predicted_class_label = class_labels[predicted_class_index]\n",
    "\n",
    "\n",
    "# print the labeld prediction\n",
    "print(\"Predicted class:\", predicted_class_label)\n",
    "print(\"Raw predictions:\", predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
